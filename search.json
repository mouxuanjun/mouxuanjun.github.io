[{"title":"关于我和这个博客","url":"/2025/05/07/about-me/","content":"个人信息笔者目前就读于杭州电子科技大学，是一名大二的学生，同时也是RoboMaster（以下简称RM）比赛的成员。我在RM25赛季负责步兵机器人的嵌入式研发，目前正在担任RM26赛季嵌入式电控组组长。\n兴趣爱好\n电子设计与DIY\n嵌入式开发：C， C++， MCU， ARM， STM32\n开源项目贡献\n足球， CS， PUBG， galgame…\n\n建站目的我很早就有建一个个人博客的想法，但是一直没空(并非没空，就是纯懒)，所以迟迟没有动手操作。现在我的比赛暂时告一段落，于是按照朋友推荐的方法，参照网上的资料搭建了你现在浏览的这个博客。\n其实我对很多好玩的东西都有浓厚的兴趣，但是由于我个人的能力问题，以及对自身控制力的不够，导致很多事情关注了但是没开始。或者开始了，遇到一个难题，然后又半途而废了。亦或者一个功能做出来了，但是背后的很多知识是没有掌握透的，于是也希望通过在这个博客分享记录，督促自己学习进步\n博客内容\n嵌入式相关知识的学习经验与成果展示\n其他一些有趣开源项目的学习记录\nACG相关内容，短评之类\n个人随记\n\n具体内容并没有一个标准，想到什么就发什么\n联系方式\nGithub: mouxuanjun\nEmail: hewenxuan040923@gmail.com\nQQ: 1583164119\n\n权限说明本博客全部内容（除特殊说明外）均采用 CC BY-NC-SA 4.0 协议进行许可。\n"},{"title":"新建一个博客以及博客维护","url":"/2026/02/12/hexo-blog/","content":"简述本文档主要记录了如何从零开始新建一个博客，以及为后续维护提供一个指导。该博客采用Hexo框架，部署到github pages，主题则为redefine\n环境\nWindows 11或Ubuntu 24.04\n\n建立第一个博客建议直接查看官方教程，会比本文档更加全面\n\nHexo官方文档\nredefine主题官方文档\n\n1. 下载并安装git\nWindows：请前往git官网下载安装包\nUbuntu：在终端执行以下命令  sudo apt update #更新软件包列表sudo apt-get install git-core -y #安装git\n输入以下命令，如果有版本号输出则表示git安装成功\n\ngit --version\n2. 下载并安装Node.js\nWindows：请前往Node.js官网下载安装包并安装。不过更建议使用NVM来管理Node.js版本，此处不过多介绍nvm的安装和使用，建议自行搜索相关教程\nUbuntu：使用如下命令安装。但是笔者首次建站是在windows上操作的，建议自行查找相关更权威的文档  sudo apt update #更新软件包列表sudo apt install -y curl #安装curlcurl -fsSL https://deb.nodesource.com/setup_23.x -o nodesource_setup.sh #下载Node.js安装脚本sudo -E bash nodesource_setup.sh #运行安装脚本sudo apt install -y nodejs #安装Node.js\n输入以下命令，如果有版本号输出则表示node.js安装成功\n\nnode -v\n3. 安装Hexo在终端执行以下命令安装Hexo\nnpm install -g hexo-cli\n输入以下命令，如果有一堆版本号输出则表示Hexo安装成功\nhexo -v\n4. 创建博客在你想要存放博客的目录下打开终端，执行以下命令\nhexo init &lt;folder&gt;cd &lt;folder&gt;npm install\n其中&lt;folder&gt;为你想要创建的博客目录名称,完成后项目文件夹将如下所示\n.├── _config.yml├── package.json├── scaffolds├── source|   ├── _drafts|   └── _posts└── themes\n具体各级文件夹分别是什么含义请自行查看官方文档，这里不需赘述\n5. 安装并启用redefine主题在终端执行以下命令\nnpm install hexo-theme-redefine@latest\n然后打开_config.yml文件，找到theme字段，将其值改为redefine\ntheme: redefine\n6. 配置redefine主题创建一个_config.redefine.yml文件(请确保文件名完全一致)，并将此处内容复制进去,此时redefine主题会自动覆盖hexo默认主题的配置项。然后具体怎么按自己喜欢的样式配置请查看官方文档，内容过多此处不再赘述。\n7. 本地预览在博客路径下的终端执行以下命令\nhexo clean #清除缓存（可选）hexo g #生成静态文件hexo s #启动本地服务器\n此时在浏览器访问http://localhost:4000即可预览博客\n8. 创建git仓库创建一个GitHub仓库，且仓库名字必须是&lt;github_username&gt;.github.io，其中&lt;github_username&gt;为你的GitHub用户名，勾选Initialize this repository with a README，然后点击Create repository按钮创建仓库。\n9. 部署到GitHub Pages在博客路径下的终端执行以下命令\nnpm install hexo-deployer-git --save #安装部署插件\n然后打开_config.yml文件，找到deploy字段，修改为如下内容\ndeploy:  type: git  repository: git@github.com:用户名/用户名.github.io.git # 替换为你的仓库地址  branch: main #如果你的默认分支是master则改为master\n然后执行以下命令部署\nhexo clean #清除缓存（可选）hexo g #生成静态文件hexo d #部署到GitHub Pages\n此时访问https://&lt;github_username&gt;.github.io即可看到你的博客上线了\n如何写文章本教程是以草稿-正文的形式教学，即先创建草稿，草稿写完后发布为正式文章。但是实际使用过程中也可以直接新建正文具体操作为把第1步中的draft改为post即可。\n1. 创建一个草稿在博客路径下的终端执行以下命令\nhexo new draft &quot;文章标题&quot;\n此时你的source/_drafts目录下会生成一个新的markdown文件，文件名为文章标题.md，打开该文件即可编辑文章内容如果要预览草稿则输入以下命令\nhexo clean #清除缓存（可选）hexo g #生成静态文件hexo server --draft #启动本地服务器（可查看草稿渲染）\n2. 发布文章在博客路径下的终端执行以下命令\nhexo publish draft &quot;文章标题&quot;\n此时你的文件会从source/_drafts目录移动到source/_posts目录下\n3. 推送到GitHub Pages在博客路径下的终端执行以下命令\nhexo clean #清除缓存（可选）hexo g #生成静态文件hexo d #部署到GitHub Pages\n\n跨平台管理博客如果你想在多台电脑或者不同的操作系统上管理你的博客，可以使用git来同步博客内容。以下是具体操作步骤\n1. 推送本地博客重要配置到远程仓库不建议使用同一个仓库的不同分支作为隔离，建议新建一个私有仓库对博客源码进行管理，在博客路径下的终端执行以下命令\ngit checkout -b &lt;branch&gt; #新建并切换分支git add . #添加所有文件git commit -m &quot;Initial commit&quot; #提交更改git remote add origin &lt;url&gt; #连接新的远程仓库git push -u origin &lt;branch&gt; #推送到远程仓库\n建议注意添加git忽略文件.gitignore隔离不同环境，内容如下\n.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/_multiconfig.yml\n\n2. 在另一台电脑上克隆仓库在另一台电脑上配置好博客需要的环境，然后在你想要存放博客的目录下打开终端，执行以下命令\ngit clone -b &lt;branch&gt; &lt;url&gt; #克隆远程仓库的指定分支cd &lt;folder&gt; #进入博客目录npm install #安装依赖# 安装依赖可使用更保险的做法：npm install --legacy-peer-deps\n结束后尝试输入hexo -v如果有内容输出则说明安装完成\n3. 修改博客内容写完后相同步骤生成并部署\nhexo clean #清除缓存（可选）hexo g #生成静态文件hexo d #部署到GitHub Pages\n4. 然后将更改推送到远程仓库git add . #添加所有文件git commit -m &quot;Update blog&quot; #提交更改git push origin &lt;branch&gt; #推送到远程仓库\n5. 更新博客环境如果你在一台电脑上修改了博客内容，那么另一台电脑需要先拉取最新的更改，然后再进行修改\ngit pull origin &lt;branch&gt; #拉取远程仓库的最新更改\n","tags":["blog","教程"]},{"title":"在Ubuntu上搭建STM32开发环境（CUBEMX+Clion+Ozone）","url":"/2025/10/05/stm32-ubuntu-clion/","content":"简述本文主要介绍ubuntu环境下使用CUBEMX+Clion+Ozone进行stm32开发的环境配置，适用于初学者。至于为啥要用Clion而不是Keil或者IAR，主要是因为Clion是跨平台的IDE，且对CMake支持很好，且免费（学生教育认证），而Keil和IAR都是收费的(但是有不收费的手段doge)，并且只能在Windows下使用。\n开发环境\nUbuntu 24.04\n\n下载并安装CUBEMX\n直接去st官网下载最新版本的CUBEMX，应该需要你登录，没有账号的按指示注册一个登录即可，下载后解压到你想放的地方即可。\n进入解压后的文件夹，执行./SetupSTM32CubeMX-x.xx.x (可能名字会有区别，反正运行安装程序就完了)，会出现一个图形化界面，按提示安装即可。\n打开下载完成的Cubemx，登录第一步注册的账号后,点击INSTALL/REMOVE下载所需软件包即可，例如我使用RoboMaster的C板，则下载STM32F4系列的软件包即可。\n\n下载并安装Clion\n直接去官网下载最新版本的Clion安装包，下载后放到任意位置(推荐放到/opt/clion目录下)解压即可。解压命令：\n\nsudo tar -xvf CLion-*.tar.gz -C /opt/clion --strip=1\n\n进入解压后的文件夹，执行./clion.sh即可打开Clion，或者直接双击clion文件\n第一次打开会提示你输入激活码，直接选择Evaluate for free即可免费试用30天，或者直接进行学生教育认证，很简单的，去Jetbrains官网按要求操作即可。\n进入Clion后，选择Settings → Build, Execution, Deployment → Toolchains，配置好C和C++编译器为/usr/bin/arm-none-eabi-gcc和/usr/bin/arm-none-eabi-g++（如果是其他路径选择对应路径即可）。这里配置是因为我clion自动查找的的gcc和g++并非arm-none-eabi版本的，导致后续编译会报错，但是我有同学是可以不用手动配置的，请更具实际情况选择。\n\n安装gcc-arm-none-eabi编译器和cmake\n输入安装命令：\n\nsudo apt updatesudo apt install gcc-arm-none-eabisudo apt install cmake -y\n\n验证安装是否成功\n\narm-none-eabi-gcc --versioncmake --version\n有内容输出即可\n安装openocd(可选)\n输入安装命令：\n\nsudo apt install openocd -y\n\n验证安装是否成功\n\nopenocd --version\n有内容输出即可\n安装JLink和Ozone\n直接去Segger官网下载最新版本的JLink和Ozone安装包解压命令：\n\nsudo apt install ./JLink_Linux_V788e_x86_64.debsudo apt install ./Ozone_Linux_V788e_x86_64.deb\n\n验证安装是否成功\n\nJLinkExe\n有内容输出即可\nOzone\n会启动Ozone即可3. 官方教学里是建议将 JLink 目录添加到环境变量中，但笔者操作时忘记这一步发现也是可以的，也可能是最新版Jlink在下载的时候会自动添加？命令如下：\necho &#x27;export PATH=$PATH:/opt/SEGGER/JLink&#x27; &gt;&gt; ~/.bashrcsource ~/.bashrc\n\n点个灯试试\n打开CUBEMX，选择New Project，选择你要使用的芯片型号，例如我使用RoboMaster的C板，则选择STM#32F4407IGH6，然后点击Start Project。\n配置相应工程，这里只是环境配置教学，就不赘述了。配置好后点击Project Manager，然后选择Toolchain/IDE为CubeIDE，然后点击Generate Code即可生成代码。这里我有个疑问，我亲测如果选择CMake，生成的代码直接编译会过不去，makefile我没试过，所以建议直接选择CubeIDE。\n用Clion打开工程文件夹，首次先右键选择cmake点击Build，如果顺利的话会生成.elf文件在Debug文件夹下。\n连接好JLink和开发板，打开Ozone，Device选择STM32F407IG，Peripheral选择/opt/SEGGER/Ozone_Vxxx/Config/Peripheral/STM32F407IG.svg，然后点击Next。在接下来的窗口中Target Interface选择SWD，并点击下面识别到的Jlink编号，点击Next，最后选择对应的elf文件，后面一路点Next即可。\n点击Start Debugging，然后点击左上角的Run按钮，点亮LED灯即可。由于Clion目前对Jlink的烧录配置过程不是很友好，所以建议直接通过在ozoneDebug的方式进行烧录和调试。\n\n","tags":["教程","stm32开发环境","ubuntu"]},{"title":"GIT的基础使用","url":"/2025/05/07/use-of-git/","content":"使用Git的原因及简单介绍\n版本管理可以跟踪代码历史，每次修改都有记录，方便回溯和恢复。可以随时回退到之前的版本，防止误操作导致数据丢失。\n团队协作多人可以同时开发，不用担心代码冲突。通过分支（Branch），不同任务可以独立开发，最后合并到主分支。\n代码备份代码托管在 GitHub、Gitee、GitLab 等平台，不怕本地数据丢失。任何地方都可以拉取代码，随时恢复开发环境。\n提高开发效率多人协作更流畅，自动合并代码，减少手动修改的麻烦。\n适用于开源和企业项目GitHub、Gitee 上的开源项目都用 Git，方便贡献代码。企业团队普遍采用 Git 管理代码，提高协作效率。简单来说：Git 能帮助你更安全、更高效地管理代码，并支持多人协作。\n\n一、下载并安装GitWindows：1. 官网下载：Git下载链接2. 安装记得换路径，不要安装到C盘。其他一路默认就可，也可以打开自动更新，添加快捷图标等。3. 安装完成后按Win+R输入cmd回车打开Windows PowerShell，输入：git --version\n输出版本号，代表安装成功。\nLinux:1. 命令行输入sudo apt install git -y\n2.完成后输入：git --version\n输出版本号，代表安装成功。\n二、配置Git1. 在任意路径右键-&gt;查看更多选项-&gt;Open Git Bash Here，打开git命令终端。2. 输入以下命令配置身份信息（建议使用真实信息）：git config --global user.name &quot;你的GitHub用户名&quot;git config --global user.email &quot;你的GitHub邮箱&quot;\n检查配置是否成功：\ngit config --list\n\n三、创建本地仓库1. 在项目目录下按第二点所说流程打开git中断，或直接输入cd 你的项目目录\n2. 输入以下命令，会在文件里创建一个.git文件，出现代表创建成功git init\n\n四、创建GitHub仓库1. 注册账号并登录GitHub2. 点击 “New Repository” ，配置仓库信息后创建一个新仓库3. 复制仓库的HTTPS或SSH地址，稍后会用到五、连接本地仓库和远程仓库有两种方式，分别是ssh和https，但是综合比较，ssh比https好很多，推荐使用。\n\nHTTPS 方式在每次 git push 或 git pull 时，都可能要求输入 GitHub 账户的用户名和密码（如果未配置凭据存储）。SSH 方式则使用 SSH Key 进行身份认证，只需要配置一次，以后就能自动认证，无需手动输入密码。\nHTTPS 认证 需要使用 GitHub 个人访问令牌（PAT），而 PAT 具有一定的权限，一旦泄露，可能带来安全隐患。SSH 认证 通过 公私钥加密，私钥存放在本地，公钥添加到 GitHub，即使公钥泄露也无法被滥用。只要私钥安全（不要泄露 id_rsa），就能保证 SSH 认证的安全性  \nSSH 方式 使用 Git 协议，默认端口 22，在某些环境下比 HTTPS 方式（走 443 端口）速度更快，尤其是 大文件传输 时表现更好。   \nHTTPS 方式 在访问私有仓库时，每次都需要身份验证，而 SSH 方式一旦设置好，访问私有仓库更方便。在某些内网或防火墙环境下，HTTPS 可能被拦截，而 SSH 可能不受影响，可以更稳定地访问 GitHub 或其他 Git 服务器。(校园网环境https基本传不出去doge)\n\nHTTPS方法git remote add origin 你的GitHub仓库地址（https地址）git remote add origin git branch -M main  # 将默认分支改为 main（可选，好习惯）\nSSH方法1. 首先要生成一个SSH Keyssh-keygen -t rsa -C &quot;你的GitHub邮箱&quot;\n一直按回车，直到生成完成   \n2. 添加SSH Key打开公钥文件，粘贴公钥并保存\ncat ~/.ssh/id_rsa.pub\n3. 把公钥添加到github中1. 点击右上角头像 → Settings\n2. 左侧栏点击 SSH and GPG keys\n3. 点击 New SSH key\n4. 粘贴公钥内容\n\n4. 测试是否成功ssh -T git@github.com\n如果成功会看到如下输出：\nHi 用户名! You&#x27;ve successfully authenticated, but GitHub does not provide shell access.\n5. 链接本地仓库和Github仓库git remote add origin 你的GitHub仓库地址（ssh地址）\n检查连接情况输入以下命令即可看到当前仓库链接的所有远程仓库，对，一个本地仓库可以链接多个远程仓库的。\ngit remote -v\n输出github仓库地址即完成连接\n六、 提交代码git add . #添加内容进暂存区git commit -m &quot;提交信息&quot; #把暂存区内容存到本地仓库git push origin main #把本地仓库更改推送到远程仓库main分支\n\n七、回退版本回退到上一个版本\n保留代码修改（常用），保留代码和暂存区\n\ngit reset --soft HEAD^\n\n丢弃修改（常用），丢弃暂存区修改，保留代码\n\ngit reset --mixed HEAD^\n\n彻底删除（危险⚠️），回退commit+暂存+修改全清除\n\ngit reset --hard HEAD^\n回退到指定版本1. 查看commit记录git log\n会看到如下输出：\ncommit a1b2c3d4e5f6g7h8... #复制这段Author: xxxDate: ...Message: 修复登录问题\n2. 回退git reset --soft a1b2c3d\n或\ngit reset --mixed a1b2c3d\n或\ngit reset --hard a1b2c3d\n撤销回退1. 查看refloggit reflog\n会列出所有的操作记录\na1b2c3d HEAD@&#123;0&#125;: reset: moving to HEAD^e5f6g7h HEAD@&#123;1&#125;: commit: 修复登录问题\n2. 滚到指定操作记录git reset --hard e5f6g7h\n\n八、 新建分支\n\n\n优点\n说明\n\n\n\n✅ 避免冲突\n每人开发自己的功能，互不干扰。\n\n\n✅ 更清晰\n分支命名可以反映开发内容\n\n\n✅ 易于管理\n可以单独审核每个人的代码\n\n\n✅ 回退方便\n如果某个人的功能不稳定，可以只回滚或暂停该分支\n\n\n具体操作为：\n\n\n\ngit switch -c feature/mywork #创建并切换到新分支\n查看是否成功：\ngit branch -vv\n如果成功会有如下输出：\n* feature/mywork  123abc [origin/feature/mywork] commit message\n九、多人协作开发基本规定：\n\nmain ：主分支，保证是稳定版本\ndev ：开发分支，用于开发新的功能\nfeature&#x2F;xxxx ：功能分支，用于开发新的功能\nhotfix&#x2F;xxxx ：bug修复分支，用于修复bug\n\n1. 克隆主仓库git clone 仓库地址\n2. 切换到dev分支git checkout dev #切换到dev分支git pull origin dev #拉取远程dev分支到本地dev分支\n3. 创建自己的功能分支git switch -c feature/xxxx #创建并切换到新分支\n4. 开发自己的功能，并提交代码到feature&#x2F;xxxx分支,推送到远端分支5. 合并到dev分支（由管理员&#x2F;审核人执行）可以在远程仓库直接操作或在本地合并：\ngit checkout dev #切换到dev分支git pull origin dev #拉取远程dev分支到本地dev分支git merge feature/xxxx #合并feature/xxxx分支到dev分支git push origin dev #把dev分支推送到远程dev分支\n6. 删除已完成分支git branch -d feature/xxxx #删除本地分支git push origin --delete feature/xxxx #删除远端分支\n\n附加VsCode推送代码图形化界面，易于操作，原理同终端。\n\n把要保留更改的文件添加到暂存区\n写提交信息，把暂存区内容加入本地仓库\n点击发布，把本地仓库修改推送到远程仓库\n\n标准格式之前教的虽然够用，但其实每次推送提交的信息格式都不是很标准，用命令行操作过于复杂，可以使用VsCode的“git-commit-pligin”插件实现，具体过程不在赘述，请自行查阅插件说明\n结束总结以上内容只是Git的最基础的使用，也只是Git的冰山一角，Git还有很多很多功能没介绍到，大家可以自己去发现，之后发现好用的功能会不定时更新此文档   \n","tags":["教程","git"]},{"title":"如何从零开始学习使用yolo","url":"/2026/02/16/yolo-learn/","content":"1. 简述本文档主要记录了如何从零开始学习使用yolo，包括环境搭建、模型训练和推理等内容。yolo主要用于目标检测任务，旋转目标检测，姿态估计，实例分割，图像分类等任务。要做这些任务，就得有对应的模型，而训练模型需要有数据集，数据集的准备和标注是一个比较繁琐的过程，本文档也会介绍一些常用的数据集和标注工具。\n2. 环境2.1 操作系统 笔者使用的是Ubuntu 24.04.3，windows环境下差别不大，按照本文步骤下载对应的安装包或资料即可，需要特殊处理的地方会在文中注明。\n2.2 显卡 笔者显卡为GeForce RTX 4060。本文只涉及nvidia显卡如果是A卡请移步相关文档或使用下述提到的CPU训练。ubuntu下可使用nvdia-smi命令查看显卡型号和驱动版本，windows下可以在设备管理器中查看显卡型号，建议使用至少6GB显存的显卡，yolo模型训练需要较大的显存，建议至少4GB以上。不过也有纯CPU训练的版本，两者仅速度差距较大，CPU训练可能需要几倍甚至几十倍的时间。\n2.3 内存 笔者的是32G内存，yolo模型训练需要较大的内存，建议至少16G以上。\n如果上述环境条件均不满足或有较大差距，可以尝试使用云服务进行训练或抓紧买个新电脑吧。\n3. 相关概念介绍3.1 YOLO版本选择\n\n图片来源于BilbilUp主@林亿饼\n\nYOLO截止目前已经出到YOLOv12(2026.2.13)，其中YOLO8和YOLO11是比较常用的版本。从图中可以明显看出，YOLO主打目标检测，然后YOLOv11和YOLOv8可以完成所有任务，故建议初学者直接学习YOLOv8或YOLOv11，后续如果需要使用其他版本。本文将以YOLOv11为例进行讲解。\n3.2 训练和推理训练和推理是两个不同的概念，训练是指使用数据集对模型进行训练，使模型能够学习到数据中的特征和规律，最终得到一个能够对新数据进行预测的模型。而推理是指使用训练好的模型对新数据进行预测，比如输入一张图片，模型会输出图片中目标的位置和类别等信息。说人话就是训练是得到模型，推理是使用模型。\n3.3 数据集你只要知道自己要预测的目标，然后找到对应的数据集就行了，常用的数据集有COCO、VOC、ImageNet等，这些数据集都包含了大量的图片和对应的标注信息，可以用于训练和评估模型。但是，如果没有人做过你要的数据集，你也可以自己收集数据并进行标注，供模型使用。\n4. 环境配置4.1 安装python环境ubuntu自带python环境，windows需要自行安装python，浏览器直接搜索下载即可，建议使用python3.8以上版本，安装完成后可以使用python --version命令查看python版本。\n4.2 安装anconda4.2.1 anconda简介anconda是一个非常好用的python环境管理工具，可以帮助我们创建和管理不同的python环境。初学者可能不知道什么是python环境，简单来说就是一个独立的空间，可以安装不同版本的python和不同的库，互不干扰。比如我有两个项目，一个需要python3.8，另一个需要python3.10，且两者需要的包版本不同，如果不使用环境管理工具，就会很麻烦，使用anconda就可以轻松解决这个问题。\n4.2.2 anconda安装下载方法可以直接进入anconda官网，选择对应的操作系统版本下载即可，但是要求登陆，其实不麻烦，因为可以跳过(doge)。或者国内最好从清华源进行下载。下载完成后按照提示安装即可，安装过程中可以选择添加anconda到环境变量，这样就可以在命令行中直接使用conda命令了。使用conda --version命令就可以查看conda版本，有输出则说明安装成功。\n4.2.3 anconda使用下面是一些基础的anconda命令：\n\nconda create -n myenv python=3.11 创建一个名为myenv的python3.11环境\nconda activate myenv 激活myenv环境\nconda deactivate 退出当前环境\nconda env list 查看所有环境\nconda remove -n yoloenv --all 删除yoloenv环境\nconda install package_name 安装包，比如conda install numpy安装numpy包,此时使用pip安装也可以，pip install package_name，不过建议使用conda安装包，因为conda会自动解决依赖问题，而pip可能会出现依赖冲突的问题。\n\n4.2.4 创建yolo环境对于此次yolo项目，我们运行如下命令创建一个yolo环境：\nconda create -n yoloenv python=3.11conda activate yoloenv\n此时前面的(base)变为(yoloenv)，说明已经成功激活了yoloenv环境。\n\n注意，本文后续安装包的命令都是在yoloenv环境下执行的，如果没有激活环境，可能会出现找不到命令或包的错误。\n\n4.3 安装pytorch先根据电脑显卡硬件选择正确的pytorch版本，可对照下表查询自己需要的版本：\n\n\n\n显卡\nPyTorch版本\n备注\n\n\n\n50系英伟达\n最新+CUDA12.8\n只能装这个\n\n\n非50系英伟达\n2.5.0+CUDA11.8\nCUDA版本低于电脑驱动CUDA版本都能用\n\n\n不带NVIDIA显卡\n2.5.0+CPU\n当初买错电脑了\n\n\n下面将以NVIDIA非50系显卡为例，安装pytorch2.5.0+CUDA11.8版本，其他版本的安装方法类似，直接替换版本号即可：\n4.3.1 检查驱动首先在命令行中输入nvidia-smi命令查看显卡型号和驱动版本，确认此时的CUDA版本大于等于11.8，否则需要更新nvidia驱动，更新驱动不再赘述。\n4.3.2 进入pytorch官网找安装命令进入pytorch官网的安装页面，搜索索引到2.5.0版本，选择对应的操作系统、包管理器、python版本和CUDA版本，官网会自动生成安装命令，比如对于ubuntu系统，使用conda包管理器，python3.11，CUDA11.8的安装命令如下：\nconda install pytorch==2.5.0 torchvision==0.20.0torchaudio==2.5.0 pytorch-cuda=12.4 -c pytorch -c nvidia\n4.3.3 执行安装命令复制命令到命令行中执行，不过安装过程可能超级超级慢，要么你可以选择耐心等待，或着更聪明的做法是使用国内镜像源进行安装，清华真是太厉害了(doge),下面是切换到清华源的命令：pip config set global.index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple，如果还是慢，可以试试连上手机热点再尝试安装。\n4.3.4 验证安装验证安装是否成功，可以在python环境中输入以下代码,如果输出为True，则说明pytorch安装成功并且可以使用CUDA进行加速了。：\nimport torchprint(torch.cuda.is_available())\n4.4 下载YOLO源码4.4.1 拉取源码去到全球最大同性交友网站GitHub上搜索ultralytics，或直接进入YOLO仓库,找对应版本，可以使用8.3.163版本，直接下载zip包解压到本地，或者使用git命令克隆仓库到本地：\ngit clone -b 8.3.163 git@github.com:ultralytics/ultralytics.git\n4.4.2 下载官方模型去到对应网站,找到V8.3.0下面的assets，其中有几百个官方欲训练好的模型，用ctrl+f搜索yolo11n， 下载yolo11n.pt模型文件，下载完成后放到YOLO源码的根目录下，后续训练和推理时会用到这个模型文件。，建议初学时可以下载所有yolo11n-xx.pt模型文件\n4.4.3 源码文件夹找到4.1中下载的yolo源码文件夹，将下载的模型文件移动到该文件夹下，文件准备工作就全部结束了\n4.4.4 下载剩余文件在yolo源码文件夹下，先激活yoloenv环境，然后执行以下命令安装剩余的依赖包：\npip install -e .\n此时所有的环境就已准备就绪了\n4.5 下载IDE对于python开发来说，纯命令行的开发可能帅，但是很傻，故绝大多数时候都需要下载一个IDE，一个好的IDE不仅能帮你补全代码，还能图形化的帮你调试代码，管理环境，大大提高开发效率。一般来说常用的两个IDE是Pycharm和宇宙最强IDEVsCode，这两个无需过多介绍,下载和安装直接去官网找对应安装包即可，Pycharm建议进行教育认证后使用专业版，功能更强大，VsCode则是完全免费的，功能也非常强大。两者区别没有特别大，凭个人爱好选择即可。\n5. 调用官方模型进行推理在IDE中(本文教学以Pycharm为例)，打开yolo源码文件夹\n5.1 查看官方图片示例新建一个Python文件，然后输入以下代码，运行后会在当前目录下生成一个runs文件夹，里面有一个detect文件夹，该文件夹中就存放了官方提供的测试图片的运行结果，里面有一些目标检测的结果，可以看到模型已经成功地对图片中的目标进行了检测，并且标注了类别和置信度。(原图就位于ultralytics/assets/bus.jpg路径下)\nfrom ultralytics import YOLOmodel = YOLO(r&#x27;yolo11n.pt&#x27;)model.predict(    source=r&quot;ultralytics/assets&quot;,    save=True,    show=False,)\n运行结果如下：\n5.2 摄像头推理在上面的代码基础上，将source参数改为0，并将save改为False，show改为True。即可使用摄像头进行推理，运行后会弹出一个窗口，显示摄像头的画面，并且对画面中的目标进行检测和标注，按q键可以退出窗口。但是你会注意到，随着时间的推移，窗口会越来越卡，甚至会崩溃，这是因为每次循环都会调用model.predict方法进行推理，而该方法会不断地占用内存，导致内存泄漏，最终导致程序崩溃。为了解决这个问题，我们需要使用stream参数来进行流式推理，这样就不会占用过多的内存了，并且使用opencv控制输出。下面是修改后的代码：\nfrom ultralytics import YOLOimport cv2model = YOLO(r&#x27;yolo11n-pose.pt&#x27;)results = model(    source=0,    stream=True,)for result in results:    plotted = result.plot()    cv2.imshow(&quot;Camera&quot;, plotted)    if cv2.waitKey(1) &amp; 0xFF == ord(&#x27;q&#x27;):        breakcv2.destroyAllWindows()\n\n注意:此时需要安装opencv库，安装命令为conda install opencv，opencv很大，安装可能持续5,6min，安装完成后再次运行上述代码即可。\n\n6.训练6.1 coco8数据集对YOLO11n模型进行训练新建一个train.py文件，输入以下代码，点击运行后就会自动从网上下载coc8数据集，并且使用yolo11n.pt模型进行训练\nfrom ultralytics import YOLOif __name__ == &#x27;__main__&#x27;:    model = YOLO(r&#x27;yolo11m.pt&#x27;)    model.train(        data=r&#x27;coco8.yaml&#x27;,        epochs=100,        imgsz=640,        batch=-1,        cache=&#x27;ram&#x27;,        workers=1,    )\n需要注意的是，第一次训练很可能失败，原因大概率就是下载数据集失败了，报错信息中有一行如下：Downloading https://ultralytics.com/assets/coco8.zip to xxx，以直接在浏览器中打开前面的链接，手动添加到后续的xxx路径下即可，不需要解压缩。然后在进行训练就可以在runs/train路径下看到训练出来的模型文件了，训练过程中会输出一些日志信息，包括训练的轮数、损失值、精度等指标，可以通过这些信息来判断模型的训练情况。但是初学者一般看不懂这些指标的含义，建议先不关注这些指标，等后续对模型有了一定的了解后再来学习这些指标的含义和作用。训练完成后会在runs/train/weights路径下生成一个best.pt文件，这个文件就是训练好的模型文件，可以用来进行推理了。\n\n上述只提到了一个训练的例子，即最简单的coco8数据集，可以自行尝试其他数据集和其他模型的训练，训练方法基本相同。\n\n6.2 数据集查看6.2.1 安装labelimg标注工具由于上述安装的python是3.11版本，与本次使用的labelimg标注工具不兼容，所以需要安装一个python3.8版本的环境来使用labelimg，安装方法同4.2.4中创建yoloenv环境的方法，命令如下：\nmamba create -n labelimgenv python=3.8conda activate labelimgenvmamba install labelimg\n6.2.2 创建classes.txt文件还是以coco8为例，找到数据集图片，数据集标签,然后在数据集标签文件夹里面新建一个名为classes.txt的文件，其中存放了数据集中所有类别的名称，每行一个类别，顺序要和数据集标签文件中的类别编号一致，比如coco8数据集中有80个类别，那么classes.txt文件中就应该有80行，每行对应一个类别的名称，具体的类别名称可以在coco8.yaml文件中找到。复制过来即可，后续在使用labelimg进行标注时会用到这个文件。\n6.2.3 使用labelimg查看coco8数据集记住上述三个内容的路径，打开命令行，输入以下命令：\nconda activate labelimgenvlabelimg xxx yyy zzz\n其中xxx是数据集图片的路径，yyy是classes.txt的路径，zzz是数据集标签的路径。中间用空格隔开，输入完成后按回车键，就会打开labelimg工具，并且自动加载数据集图片和标签了，可以在工具中查看数据集的图片和标签了。\n6.3 加速训练训练效率仅能反映训练的快慢，对最终的训练结果没有影响，一个高效的训练一般具有以下特征：\n\nCUDA利用率有高又稳\n所有资源利用率都不到100%要提高训练效率，需要从cpu、显卡、内存等多个方面进行优化，下面是一些常用的优化方法：\n\n6.3.1 调整imgsz大小一次训练时，需要先将图片在保持原长宽比不变的前提下缩放到指定的大小，然后再统一投喂。默认是640x640，如果显卡显存较小，可以将imgsz适当调小，这样就可以减少每次训练时占用的显存了，从而提高训练效率。注意的是imgsz必须是32的倍数，否则会报错。需要注意的是，此参数不仅会影响训练效率，还会影响训练结果，过大的imgsz严重影响训练效率，过小的imgsz可能会导致模型无法学习到足够的特征，从而导致训练结果不理想，所以需要根据实际情况进行调整。举个最简单的例子，如果你把一张图片压缩到1&#x2F;10,那么实际图片已经变成一堆马赛克了，训练出来的模型也就只能帮你预测一些马赛克了，遇到正常的图片就完全不行了。放大也是同理。但是一般默认即可，除非原本数据集就过大就过小。\n6.3.2 调整batch大小batch大小指的是每次训练时投喂多少张图片，默认是16。如果样本较大，或者显卡现存较小，无法一次性投喂全部图片，故需要对样本进行分批，适当调小batch大小，这样就可以减少每次训练时占用的显存了，从而提高训练效率。如果一开始不知道要设置为多少，可以先设置为-1,此时在训练前会多一个autobatch环节，可以自动为你找到合适的batch大小。\n6.3.3 使用缓存把cache=False改为cache=&#39;ram&#39;，这样YOLO就会在训练开始前把所有数据加载到内存中，再全部缩放好，随后等到对应批次时直接投喂给模型就行，唯一缺点就是占用内存较大，如果内存较小，可能会导致系统崩溃，所以需要根据实际情况进行调整。一般在数据集尺寸较大时使用缓存可以大大提高训练效率。\n6.3.4 调整workers数量由于打包是随机的，故没有办法提前打包，打包本身也需要一定时间，如果打包慢了也会影响投喂，解决方法就是多开几个线程来负责打包。但是同样的，过多的线程也会占用过多的系统资源，一般建议选择一个较小的值。\n6.3.5 优化源码YOLO是基于python编写的，故如果你本身编程语言能力较强，可以尝试优化源码来提高训练效率，但是笔者在此方面是菜鸡，所以不敢妄言，如果有巨佬能提供一些优化建议，欢迎在评论区留言。或者是优化Pytorch和CUDA，那就更牛逼了，如果你还不满足，可以直接修改nvidia驱动或直接硬件魔改显卡，相信你自己。\n7. 训练一个自己的模型7.1 数据集准备在弄明白自己要预测的目标后，你可以先找找网上有没有对应的数据集，如果有的话就直接下载使用，如果没有的话就需要自己收集数据并进行标注了，数据集的准备和标注是一个比较繁琐的过程。\n7.1.1 去网上找现成的数据集可以直接在搜索引擎中搜索你要预测的目标+数据集，或者现在直接使用AI帮你查找，关键词为：预测目标+任务类型+YOLO模型，AI给出来的多数内容无法直接使用，但是它会推荐一些比较出名的数据集就算无法直接使用也可以尝试搜索AI给出的结果，说不定能有意想不到的惊喜。推荐两个网站，我也是在网上看别人推荐的，请自行筛选：roboflow和kaggle,如果找到你需要的数据集，那么恭喜你了，最折磨人的标数据集部分你就可以跳过了。直接看7.1.2即可。\n7.1.2 现成数据集使用当你找到正确格式的数据集后，下载其对应的ZIP，建议统一放到源码里的datasets文件夹下，然后把数据集自带的xx.yaml文件复制到ultralytics/cfg文件夹下，然后查看以下内容是否正确：\n\npath 指向数据集所在地址\ntrain 指向数据训练集地址\nval 指向数据验证集地址\ntest 指向数据测试集地址(可以没有测试集)\nnames 指明对应数据集标签的名称，顺序不可更改！！！     \n其他都无所谓了数据集准备部分你就到此为之了，接下来直接跳到训练部分即可。\n\n7.1.3 自己拍摄数据集如果你是比较悲惨的那批人，网上没有现成好的开源数据集，例如参加一些大学生竞赛的队员，多数情况都需要你自己来制作自己的数据集，关于材料准备可以尝试以下方法：先随机在不同环境和灯光环境下给目标的各个姿态拍摄几段视频，然后从视频中随机提取n张图片。这里分享一个可以自动完成此过程的脚本：\nimport cv2import osfrom pathlib import Pathfrom tqdm import tqdmdef extract_frames(frames_per_second=2):    # 1. 定义路径    base_dir = os.path.dirname(os.path.abspath(__file__))    video_dir = os.path.join(base_dir, &quot;videos&quot;)    output_dir = os.path.join(base_dir, &quot;images&quot;)    # 2. 检查并创建输出目录    if not os.path.exists(output_dir):        os.makedirs(output_dir)        print(f&quot;已创建输出文件夹: &#123;output_dir&#125;&quot;)    # 3. 获取所有 .mp4 文件    if not os.path.exists(video_dir):        print(f&quot;错误: 找不到视频文件夹 &#123;video_dir&#125;&quot;)        return    video_files = [f for f in os.listdir(video_dir) if f.lower().endswith(&#x27;.mp4&#x27;)]    if not video_files:        print(&quot;在 videos 文件夹下没有找到 .mp4 文件。&quot;)        return    print(f&quot;找到 &#123;len(video_files)&#125; 个视频文件，准备开始处理...&quot;)    print(f&quot;提取频率: 每秒 &#123;frames_per_second&#125; 帧&quot;)    print(&quot;-&quot; * 30)    # 4. 遍历处理每个视频    for video_file in video_files:        video_path = os.path.join(video_dir, video_file)        video_name_stem = Path(video_file).stem        # 打开视频        cap = cv2.VideoCapture(video_path)        if not cap.isOpened():            print(f&quot;无法打开视频: &#123;video_file&#125;&quot;)            continue        # 获取视频信息        original_fps = cap.get(cv2.CAP_PROP_FPS)        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))        if frames_per_second &gt; original_fps:            print(f&quot;警告: 目标帧率 (&#123;frames_per_second&#125;) 高于视频原始帧率 (&#123;original_fps&#125;)，将提取所有帧。&quot;)            frame_interval = 1        else:            frame_interval = int(original_fps / frames_per_second)            if frame_interval == 0: frame_interval = 1        # 设置进度条        pbar = tqdm(total=total_frames, desc=f&quot;处理 &#123;video_file&#125;&quot;, unit=&quot;帧&quot;)        frame_count = 0        saved_count = 1  # 图片序号从1开始        while True:            success, frame = cap.read()            if not success:                break            # 判断是否需要保存当前帧            if frame_count % frame_interval == 0:                # 构造图片文件名: xxx_yyyy.jpg (yyyy 为4位数字，如 0001)                output_filename = f&quot;&#123;video_name_stem&#125;_&#123;saved_count:04d&#125;.jpg&quot;                output_path = os.path.join(output_dir, output_filename)                # 保存图片                cv2.imwrite(output_path, frame)                saved_count += 1            frame_count += 1            pbar.update(1)        cap.release()        pbar.close()    print(&quot;-&quot; * 30)    print(&quot;所有视频处理完成！&quot;)if __name__ == &quot;__main__&quot;:    # --- 配置区域 ---    # 在这里修改每秒你想提取几帧    TARGET_FPS = 3    # ----------------    extract_frames(frames_per_second=TARGET_FPS)\n\n使用该脚本前需保证文件路径下有 videos 文件夹，其中存放视频格式为.mp4.点击运行则会把全部输出的图片按统一格式存放到images文件夹中，如果数量过多或过少，可以适当调整其中的参数。眼尖的同学就会发现改脚本完全由AI编写，对于这种简单小功能直接让AI来写还是非常方便和快捷的，后续类似的功能就不再直接给代码了，大家各自拷打自己的AI让它写出能用的代码即可。\n\n7.1.4 数据集标注数据集准备好了之后就需要进行标注了，标注的工具有很多，比如labelimg、labelme、xlabeling等，初学者可以尝试使用labelimg，安装方法和打开方法上述有提到。具体步骤如下：\n\n新建一个labels文件夹，用于存放标注文件\n在labels文件夹中新建一个classes.txt文件用于存放类别名称，根据需要一行一个类别，顺序要和数据集标签文件中的类别编号一致，尽量使用英文名称且尽可能不发生歧义的情况下短小一点。\n按6.2.3中教学的方法打开labelimg工具，加载数据集图片和标签。\n标注前注意标签格式是否为YOLO，如果是的话就可以开始愉快的标数据集了。A``D可以快速切换图片，W快速创建新的框，框定好后选择对应的标签即可。\n标完一张图片记得保存，以此类推，直到全部标注完成。标\n\n7.1.5 快速标注方法例如有100张图片，一张一张的标注可能会比较麻烦，这时就会想到一个办法，先提取其中一部分图片，比如20张，进行标注，然后使用这20张图片训练一个模型，随后使用这个模型对剩余的80张图片进行推理，得到的结果可能不太准确，但可以作为一个初始的标注结果，接下来只需要对这些结果进行修改和完善即可，这样就可以大大减少标注的工作量了。这个方法虽然有点麻烦，但是对于一些比较大的数据集来说是非常有用的，可以大大提高标注效率。至于怎么挑选图片就不必多说了，大家各自拷打自己的AI让它写出能用的代码即可。注意在使用小模型的时候，要把save_txt改为True。\n7.1.6 数据集归类当你准备好了数据集，并且标注好了之后，就需要把数据集按照一定的格式进行归类了，YOLO要求的数据集格式如下：\n├── images/│   ├── train/│   ├── val/│   └── test/└── labels/    ├── train/    ├── val/    └── test/\n其中images文件夹下存放图片，labels文件夹下存放对应的标签文件，train、val、test分别存放训练集、验证集和测试集的图片和标签，注意图片和标签的文件名要保持一致。此过程还是可以用AI编写脚本完成，请自行鞭策AI。\n7.1.7 数据集yaml文件数据集准备好了之后就需要创建一个yaml文件来描述数据集了，yaml文件的内容主要包括数据集的路径、类别名称等信息，YOLO会根据这个yaml文件来加载数据集进行训练和推理，yaml文件的格式如下：\npath: ../datasettrain: images/trainval: images/valtest: images/testnames:  0: class1  1: class2  2: class3\n其中path指向数据集的根目录，train、val、test分别指向训练集、验证集和测试集的图片路径，names是一个字典，键是类别编号，值是对应的类别名称，注意类别名称的顺序要和标签文件中的类别编号一致。创建好yaml文件后，就可以在训练和推理时指定这个yaml文件来加载数据集了。恭喜你，到这里已经创建好了一个自己的数据集了。\n7.2 模型训练按照 6. 训练提到的内容进行训练即可，此时使用的就不再是coco数据集了，而是你自己准备的数据集了，需要注意的是，YOLO现在有非常多的模型，但是不同的模型对不同的数据集跑出来的效果可能会有很大的差别，所以在训练时建议尝试所有感兴趣的模型，看看哪个模型在你的数据集上表现最好，当然了，如果你对模型的原理有一定的了解的话，也可以根据数据集的特点来选择合适的模型进行训练，这样可能会更快地得到一个比较好的结果。一般来说，YOLOv8是一个最常被使用的版本，适用于大多数任务，如果你不确定选择哪个版本，可以先从YOLOv8开始尝试，后续如果需要使用其他版本再进行调整即可。\n7.3 模型推理训练完成后会在runs/train/weights路径下生成一个best.pt文件，这个文件就是训练好的模型文件，可以用来进行推理了，推理的方法和前面提到的调用官方模型进行推理的方法基本相同，只需要把模型文件路径改为训练好的模型文件路径即可，如果发现效果不理想再调整参数进行重新训练即可。当然，许多时候优化训练参数真不如加大训练集的数量和场景\n8. 模型评估8.1 直观评估训练完成后，可以使用训练好的模型对验证集或测试集进行推理，看看模型的预测结果和真实标签的差距。或者更推荐的是直接用视频进行验证效果更容易看出来，尤其是出现不同场景下的预测效果。\n8.2 框相近程度IoU假设实际预测的框为A，真实标签的框为B，$S_{A \\cap B}$表示A和B的交集区域，$S_{A \\cup B}$表示A和B的并集区域，则IoU可以表示为：$$IoU &#x3D; \\frac{S_{A \\cap B}}{S_{A \\cup B}}$$也可以比较直观的发现：IoU的值越大，说明预测的框和真实标签的框越接近，反之则说明预测的框和真实标签的框越远离。一般来说，IoU的值在0到1之间，通常会设置一个阈值，常用阈值为0.5，如果IoU的值大于等于这个阈值，就认为预测的框是正确的，否则就认为预测的框是错误的。\n8.3 TP、FP、FN\nTP（True Positive）真正例，如IoU算法中提到的，如果预测的框和真实标签的框的IoU值大于等于设定的阈值，那么就认为这个预测是正确的，也就是TP。\nFP（False Positive）假正例，与TP相反，如果预测的框和真实标签的框的IoU值小于设定的阈值，那么就认为这个预测是错误的，也就是FP。\nFN（False Negative）假反例,通俗的来讲就是没找的真实结果。\nTP+FP就是预测结果总数TP+FN就是实际结果总数\n\n\n\n8.4 精确率、召回率和F1分数\n精确率（Precision）指的是在所有被预测为正例的样本中，真正例的比例。计算公式为：$$Precision &#x3D; \\frac{TP}{TP + FP}&#x3D;\\frac{正确预测}{预测总数}$$精确率越高，说明预测结果里很多都对了。\n召回率（Recall）指的是在所有实际为正例的样本中，被正确预测为正例的比例。计算公式为：$$Recall &#x3D; \\frac{TP}{TP + FN}&#x3D;\\frac{正确预测}{真实总数} $$召回率越高，说明越多的真实结果被找到了。\nF1分数（F1 Score）是精确率和召回率的调和平均数，计算公式为：$$F1 &#x3D; 2 * \\frac{Precision * Recall}{Precision + Recall}$$F1分数越高，说明模型在精确率和召回率之间取得了更好的平衡。\n这三个是最常见的评估指标，都是越接近1越好。\n\n\n\n8.5 预测过程8.5.1 一次预测的三个步骤\n预处理：将输入的图片进行缩放、归一化等处理，使其符合模型的输入要求。\n模型推理：将预处理后的图片输入到训练好的模型中，模型会输出一些预测结果，包括预测的框的位置、类别和置信度等信息。\n后处理：对模型的输出结果进行后处理，去除掉大部分多余的预测结果，并把图还原成原来的大小。此时后处理是我们此时需要重点关注的。\n\n8.5.2 后处理\n过滤掉置信度较低的预测结果(conf)：模型会输出每个预测结果的置信度，表示模型对这个预测结果的信心程度，通常会设置一个置信度阈值,一般取0.25，只有当预测结果的置信度大于等于这个阈值时，才会保留这个预测结果，否则就会被过滤掉\n非极大值抑制(NMS)：在过滤掉置信度较低的预测结果后，可能会有一些预测结果的位置非常接近，甚至重叠，这时就需要使用非极大值抑制来去除掉这些重复的预测结果。简单来说就是预测相同目标的，只保留置信度最高的一个结果，其他的都丢掉了。注意的是：IoU的值会影响此过程，IoU的值越大，说明预测结果之间的重叠程度越高，那么就越有可能被认为是重复的预测结果，从而被去除掉。\n限制预测结果(max_det)：限制预测结果总数不要超过设定值，一般都很大。\n\n8.5.3 影响conf越小，预测出来的结果越多，如果把conf设置为0,相当于跳过来去弱，此时精确率P会很小，因为预测出来的结果很多，但是真实的数量又是有限的，但是召回率R有会很大，因为结果很多，所以有可能瞎猫碰到死耗子，一个小概率的框猜对结果。如果把conf稍微调高一点，那就会把一部分低概率的排除掉，低概率的结果大概率都是错误的，所以精确率P会提高，但是召回率R会降低，因为上述提到的瞎猫碰到死耗子的正确的结果被排除掉了。总之，conf的值需要根据实际情况进行调整，找到一个合适的平衡点，既能保证预测结果的数量，又能保证预测结果的质量。\n8.6 PR曲线，mAP50,mAP50-95\nPR曲线：PR曲线是精确率和召回率之间的关系的曲线图，通过PR曲线可以直观地看到模型在不同的置信度阈值下的表现，PR曲线越接近右上角，说明模型的表现越好。PR曲线和坐标轴包围的面积就是AP值，AP值越大，说明模型在单一类别的表现越好。mAP就是所有类别的AP值的平均值，mAP越大，说明模型在所有类别上的表现越好。\n潜台词为：能找到一个对应的conf值，使P和R都尽可能的高，那么这个模型就比较好了。\n\n\nmAP50：mAP50是指在IoU阈值为0.5时的平均精确率，mAP50越高，说明模型在IoU阈值为0.5时的表现越好。\nmAP50-95(平均平均平均精度)：mAP50-95是指在IoU阈值从0.5到0.95之间的平均精确率，mAP50-95越高，说明模型在IoU阈值从0.5到0.95之间的表现越好。\n\n8.7 总结通过上述描述我们就知道了描述模型好坏的4个重要指标：精确率P、召回率R、mAP50和mAP50-95，通常来说，模型的表现越好，这四个指标的值就越高，但是在实际应用中，这四个指标之间可能会存在一定的权衡关系，需要合理取舍。\n未完待续由于笔者也是刚开始接触深度学习与计算机视觉，所以在写这篇文章的时候也是边学边写的，后续等积攒一定调试训练经验之后会继续更新这篇文章或者写一篇新的文章来分享一些训练和调试模型的经验和技巧，敬请期待。\n","tags":["教程","深度学习","计算机视觉"]}]